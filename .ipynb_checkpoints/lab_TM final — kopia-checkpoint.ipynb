{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np\n",
    "from tqdm import tqdm\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zapewnienie powtarzalności wyników\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "tensorflow.random.set_seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>talk_id</th>\n",
       "      <th>title</th>\n",
       "      <th>speaker_1</th>\n",
       "      <th>all_speakers</th>\n",
       "      <th>occupations</th>\n",
       "      <th>about_speakers</th>\n",
       "      <th>views</th>\n",
       "      <th>recorded_date</th>\n",
       "      <th>published_date</th>\n",
       "      <th>event</th>\n",
       "      <th>native_lang</th>\n",
       "      <th>available_lang</th>\n",
       "      <th>comments</th>\n",
       "      <th>duration</th>\n",
       "      <th>topics</th>\n",
       "      <th>related_talks</th>\n",
       "      <th>url</th>\n",
       "      <th>description</th>\n",
       "      <th>transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>870</th>\n",
       "      <td>1119</td>\n",
       "      <td>The antidote to apathy</td>\n",
       "      <td>Dave Meslin</td>\n",
       "      <td>{0: 'Dave Meslin'}</td>\n",
       "      <td>{0: ['artist and organizer']}</td>\n",
       "      <td>{0: 'Dave Meslin is a \"professional rabble-rou...</td>\n",
       "      <td>1869774</td>\n",
       "      <td>2010-10-10</td>\n",
       "      <td>2011-04-12</td>\n",
       "      <td>TEDxToronto 2010</td>\n",
       "      <td>en</td>\n",
       "      <td>['ar', 'bg', 'cs', 'de', 'el', 'en', 'es', 'et...</td>\n",
       "      <td>236.0</td>\n",
       "      <td>425</td>\n",
       "      <td>['TEDx', 'collaboration', 'community', 'cultur...</td>\n",
       "      <td>{1825: 'Why mayors should rule the world', 815...</td>\n",
       "      <td>https://www.ted.com/talks/dave_meslin_the_anti...</td>\n",
       "      <td>Local politics -- schools, zoning, council ele...</td>\n",
       "      <td>How often do we hear that people just don't ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>929</th>\n",
       "      <td>1183</td>\n",
       "      <td>Try something new for 30 days</td>\n",
       "      <td>Matt Cutts</td>\n",
       "      <td>{0: 'Matt Cutts'}</td>\n",
       "      <td>{0: ['technologist']}</td>\n",
       "      <td>{0: 'An early employee at Google, Matt Cutts w...</td>\n",
       "      <td>12481445</td>\n",
       "      <td>2011-03-03</td>\n",
       "      <td>2011-07-01</td>\n",
       "      <td>TED2011</td>\n",
       "      <td>en</td>\n",
       "      <td>['ar', 'arq', 'az', 'be', 'bg', 'bn', 'bs', 'c...</td>\n",
       "      <td>916.0</td>\n",
       "      <td>207</td>\n",
       "      <td>['culture', 'success']</td>\n",
       "      <td>{947: 'Keep your goals to yourself', 282: 'Wha...</td>\n",
       "      <td>https://www.ted.com/talks/matt_cutts_try_somet...</td>\n",
       "      <td>Is there something you've always meant to do, ...</td>\n",
       "      <td>A few years ago, I felt like I was stuck in a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1669</th>\n",
       "      <td>1972</td>\n",
       "      <td>Be passionate. Be courageous. Be your best.</td>\n",
       "      <td>Gabby Giffords and Mark Kelly</td>\n",
       "      <td>{0: 'Gabby Giffords and Mark Kelly'}</td>\n",
       "      <td>{0: ['former u.s. representative and nasa astr...</td>\n",
       "      <td>{0: 'After Rep. Gabby Giffords was wounded by ...</td>\n",
       "      <td>1134014</td>\n",
       "      <td>2014-03-21</td>\n",
       "      <td>2014-04-11</td>\n",
       "      <td>TED2014</td>\n",
       "      <td>en</td>\n",
       "      <td>['ar', 'el', 'en', 'es', 'fa', 'fr', 'he', 'hu...</td>\n",
       "      <td>80.0</td>\n",
       "      <td>1128</td>\n",
       "      <td>['violence', 'guns']</td>\n",
       "      <td>{1566: 'A story about knots and surgeons', 104...</td>\n",
       "      <td>https://www.ted.com/talks/gabby_giffords_and_m...</td>\n",
       "      <td>On January 8, 2011, Congresswoman Gabby Giffor...</td>\n",
       "      <td>Pat Mitchell: That day, January 8, 2011, began...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>701</th>\n",
       "      <td>921</td>\n",
       "      <td>A headset that reads your brainwaves</td>\n",
       "      <td>Tan Le</td>\n",
       "      <td>{0: 'Tan Le'}</td>\n",
       "      <td>{0: ['entrepreneur']}</td>\n",
       "      <td>{0: \"Tan Le is the founder &amp; CEO of Emotiv, a ...</td>\n",
       "      <td>2741071</td>\n",
       "      <td>2010-07-16</td>\n",
       "      <td>2010-07-21</td>\n",
       "      <td>TEDGlobal 2010</td>\n",
       "      <td>en</td>\n",
       "      <td>['ar', 'bg', 'cs', 'da', 'de', 'el', 'en', 'es...</td>\n",
       "      <td>595.0</td>\n",
       "      <td>636</td>\n",
       "      <td>['brain', 'computers', 'design', 'entertainmen...</td>\n",
       "      <td>{685: 'The thrilling potential of SixthSense t...</td>\n",
       "      <td>https://www.ted.com/talks/tan_le_a_headset_tha...</td>\n",
       "      <td>Tan Le's astonishing new computer interface re...</td>\n",
       "      <td>Up until now, our communication with machines ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2307</th>\n",
       "      <td>2663</td>\n",
       "      <td>To solve old problems, study new species</td>\n",
       "      <td>Alejandro Sánchez Alvarado</td>\n",
       "      <td>{0: 'Alejandro Sánchez Alvarado'}</td>\n",
       "      <td>{0: ['developmental and regeneration biologist']}</td>\n",
       "      <td>{0: 'Alejandro Sánchez Alvarado wants to under...</td>\n",
       "      <td>1336563</td>\n",
       "      <td>2016-08-19</td>\n",
       "      <td>2017-01-12</td>\n",
       "      <td>TEDxKC</td>\n",
       "      <td>en</td>\n",
       "      <td>['ar', 'bg', 'ca', 'de', 'el', 'en', 'es', 'fr...</td>\n",
       "      <td>46.0</td>\n",
       "      <td>759</td>\n",
       "      <td>['animals', 'biodiversity', 'beauty', 'biology...</td>\n",
       "      <td>{206: 'Underwater astonishments', 2390: 'The s...</td>\n",
       "      <td>https://www.ted.com/talks/alejandro_sanchez_al...</td>\n",
       "      <td>Nature is wonderfully abundant, diverse and my...</td>\n",
       "      <td>For the past few years, I've been spending my ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      talk_id                                        title  \\\n",
       "870      1119                       The antidote to apathy   \n",
       "929      1183                Try something new for 30 days   \n",
       "1669     1972  Be passionate. Be courageous. Be your best.   \n",
       "701       921         A headset that reads your brainwaves   \n",
       "2307     2663     To solve old problems, study new species   \n",
       "\n",
       "                          speaker_1                          all_speakers  \\\n",
       "870                     Dave Meslin                    {0: 'Dave Meslin'}   \n",
       "929                      Matt Cutts                     {0: 'Matt Cutts'}   \n",
       "1669  Gabby Giffords and Mark Kelly  {0: 'Gabby Giffords and Mark Kelly'}   \n",
       "701                          Tan Le                         {0: 'Tan Le'}   \n",
       "2307     Alejandro Sánchez Alvarado     {0: 'Alejandro Sánchez Alvarado'}   \n",
       "\n",
       "                                            occupations  \\\n",
       "870                       {0: ['artist and organizer']}   \n",
       "929                               {0: ['technologist']}   \n",
       "1669  {0: ['former u.s. representative and nasa astr...   \n",
       "701                               {0: ['entrepreneur']}   \n",
       "2307  {0: ['developmental and regeneration biologist']}   \n",
       "\n",
       "                                         about_speakers     views  \\\n",
       "870   {0: 'Dave Meslin is a \"professional rabble-rou...   1869774   \n",
       "929   {0: 'An early employee at Google, Matt Cutts w...  12481445   \n",
       "1669  {0: 'After Rep. Gabby Giffords was wounded by ...   1134014   \n",
       "701   {0: \"Tan Le is the founder & CEO of Emotiv, a ...   2741071   \n",
       "2307  {0: 'Alejandro Sánchez Alvarado wants to under...   1336563   \n",
       "\n",
       "     recorded_date published_date             event native_lang  \\\n",
       "870     2010-10-10     2011-04-12  TEDxToronto 2010          en   \n",
       "929     2011-03-03     2011-07-01           TED2011          en   \n",
       "1669    2014-03-21     2014-04-11           TED2014          en   \n",
       "701     2010-07-16     2010-07-21    TEDGlobal 2010          en   \n",
       "2307    2016-08-19     2017-01-12            TEDxKC          en   \n",
       "\n",
       "                                         available_lang  comments  duration  \\\n",
       "870   ['ar', 'bg', 'cs', 'de', 'el', 'en', 'es', 'et...     236.0       425   \n",
       "929   ['ar', 'arq', 'az', 'be', 'bg', 'bn', 'bs', 'c...     916.0       207   \n",
       "1669  ['ar', 'el', 'en', 'es', 'fa', 'fr', 'he', 'hu...      80.0      1128   \n",
       "701   ['ar', 'bg', 'cs', 'da', 'de', 'el', 'en', 'es...     595.0       636   \n",
       "2307  ['ar', 'bg', 'ca', 'de', 'el', 'en', 'es', 'fr...      46.0       759   \n",
       "\n",
       "                                                 topics  \\\n",
       "870   ['TEDx', 'collaboration', 'community', 'cultur...   \n",
       "929                              ['culture', 'success']   \n",
       "1669                               ['violence', 'guns']   \n",
       "701   ['brain', 'computers', 'design', 'entertainmen...   \n",
       "2307  ['animals', 'biodiversity', 'beauty', 'biology...   \n",
       "\n",
       "                                          related_talks  \\\n",
       "870   {1825: 'Why mayors should rule the world', 815...   \n",
       "929   {947: 'Keep your goals to yourself', 282: 'Wha...   \n",
       "1669  {1566: 'A story about knots and surgeons', 104...   \n",
       "701   {685: 'The thrilling potential of SixthSense t...   \n",
       "2307  {206: 'Underwater astonishments', 2390: 'The s...   \n",
       "\n",
       "                                                    url  \\\n",
       "870   https://www.ted.com/talks/dave_meslin_the_anti...   \n",
       "929   https://www.ted.com/talks/matt_cutts_try_somet...   \n",
       "1669  https://www.ted.com/talks/gabby_giffords_and_m...   \n",
       "701   https://www.ted.com/talks/tan_le_a_headset_tha...   \n",
       "2307  https://www.ted.com/talks/alejandro_sanchez_al...   \n",
       "\n",
       "                                            description  \\\n",
       "870   Local politics -- schools, zoning, council ele...   \n",
       "929   Is there something you've always meant to do, ...   \n",
       "1669  On January 8, 2011, Congresswoman Gabby Giffor...   \n",
       "701   Tan Le's astonishing new computer interface re...   \n",
       "2307  Nature is wonderfully abundant, diverse and my...   \n",
       "\n",
       "                                             transcript  \n",
       "870   How often do we hear that people just don't ca...  \n",
       "929   A few years ago, I felt like I was stuck in a ...  \n",
       "1669  Pat Mitchell: That day, January 8, 2011, began...  \n",
       "701   Up until now, our communication with machines ...  \n",
       "2307  For the past few years, I've been spending my ...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_df = pd.read_csv(\"ted_talks_en.csv\")\n",
    "raw_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4005, 19)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4005 entries, 0 to 4004\n",
      "Data columns (total 19 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   talk_id         4005 non-null   int64  \n",
      " 1   title           4005 non-null   object \n",
      " 2   speaker_1       4005 non-null   object \n",
      " 3   all_speakers    4001 non-null   object \n",
      " 4   occupations     3483 non-null   object \n",
      " 5   about_speakers  3502 non-null   object \n",
      " 6   views           4005 non-null   int64  \n",
      " 7   recorded_date   4004 non-null   object \n",
      " 8   published_date  4005 non-null   object \n",
      " 9   event           4005 non-null   object \n",
      " 10  native_lang     4005 non-null   object \n",
      " 11  available_lang  4005 non-null   object \n",
      " 12  comments        3350 non-null   float64\n",
      " 13  duration        4005 non-null   int64  \n",
      " 14  topics          4005 non-null   object \n",
      " 15  related_talks   4005 non-null   object \n",
      " 16  url             4005 non-null   object \n",
      " 17  description     4005 non-null   object \n",
      " 18  transcript      4005 non-null   object \n",
      "dtypes: float64(1), int64(3), object(15)\n",
      "memory usage: 594.6+ KB\n"
     ]
    }
   ],
   "source": [
    "raw_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oczyszczanie danych\n",
    "* Usunięcie zbędnych kolumn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['talk_id',\n",
       " 'title',\n",
       " 'speaker_1',\n",
       " 'all_speakers',\n",
       " 'occupations',\n",
       " 'about_speakers',\n",
       " 'views',\n",
       " 'recorded_date',\n",
       " 'published_date',\n",
       " 'event',\n",
       " 'native_lang',\n",
       " 'available_lang',\n",
       " 'comments',\n",
       " 'duration',\n",
       " 'topics',\n",
       " 'related_talks',\n",
       " 'url',\n",
       " 'description',\n",
       " 'transcript']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_cols = [col for col in raw_df.columns]\n",
    "all_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_cols = ['title','duration','topics','description','transcript']\n",
    "del_cols = [item for item in all_cols if item not in left_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in del_cols:\n",
    "    del raw_df[col]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Tablice jako string w DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"['alternative energy', 'cars', 'climate change', 'culture', 'environment', 'global issues', 'science', 'sustainability', 'technology']\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_df[\"topics\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "topics = [ast.literal_eval(topic[\"topics\"]) for topic in raw_df.iloc]\n",
    "raw_df[\"topics\"] = topics "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Usunięcie kategorii poniżej 400 wystąpień"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categories count: 457\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "categories = defaultdict(int)\n",
    "length = range(raw_df[\"topics\"].shape[0])\n",
    "for i in length:\n",
    "    for j in topics[i]:\n",
    "        categories[j] += 1\n",
    "print(f\"Categories count: {len(categories)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "science          993\n",
      "technology       979\n",
      "culture          680\n",
      "global issues    574\n",
      "society          557\n",
      "design           518\n",
      "social change    512\n",
      "animation        487\n",
      "business         443\n",
      "health           442\n",
      "history          406\n",
      "dtype: int64\n",
      "Categories count: 11\n"
     ]
    }
   ],
   "source": [
    "more_than = 400\n",
    "cat_series = pd.Series(categories)\n",
    "cat_series = cat_series[cat_series>=more_than].sort_values(ascending = False)\n",
    "try:\n",
    "    to_del = [\"TEDx\",\"TED-Ed\"]\n",
    "    cat_series  = cat_series.drop(to_del)\n",
    "except:\n",
    "    print(\"Lack cats to del.\")\n",
    "\n",
    "print(cat_series)\n",
    "num_classes = len(cat_series)\n",
    "print(f\"Categories count: {num_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_400 = cat_series.index\n",
    "for row_num,row in enumerate(raw_df.iloc):\n",
    "    intersect = list(set(row[\"topics\"]) & set(cat_400))\n",
    "    if intersect:\n",
    "        raw_df.at[row_num, \"topics\"] = intersect\n",
    "    else:\n",
    "        raw_df = raw_df.drop([row_num])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Wagi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#weight = number_of_all_samples / (2 * number_of_samples_cl1)\n",
    "all_sampl_num = cat_series.sum()\n",
    "class_weight = {}\n",
    "for num, cat in enumerate(cat_series):\n",
    "    class_weight[num] = (all_sampl_num / (2 * cat))\n",
    "\n",
    "num_classes = len(class_weight.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weight_norm = [weight/max(class_weight.values()) for weight in class_weight.values()]\n",
    "for num, weight in enumerate(class_weight_norm):\n",
    "    class_weight[num] = weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Kategorie jako one-hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df = raw_df.reset_index()\n",
    "\n",
    "for row_num in range(raw_df.shape[0]):\n",
    "    temp_cols = raw_df.loc[row_num, \"topics\"]\n",
    "    raw_df.loc[row_num, temp_cols] = 1\n",
    "\n",
    "df = raw_df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df.drop([\"title\", \"duration\", \"topics\", \"description\", \"transcript\", \"index\"], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>title</th>\n",
       "      <th>duration</th>\n",
       "      <th>topics</th>\n",
       "      <th>description</th>\n",
       "      <th>transcript</th>\n",
       "      <th>science</th>\n",
       "      <th>technology</th>\n",
       "      <th>global issues</th>\n",
       "      <th>culture</th>\n",
       "      <th>health</th>\n",
       "      <th>business</th>\n",
       "      <th>design</th>\n",
       "      <th>social change</th>\n",
       "      <th>history</th>\n",
       "      <th>animation</th>\n",
       "      <th>society</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Averting the climate crisis</td>\n",
       "      <td>977</td>\n",
       "      <td>[science, technology, global issues, culture]</td>\n",
       "      <td>With the same humor and humanity he exuded in ...</td>\n",
       "      <td>Thank you so much, Chris. And it's truly a gre...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>The best stats you've ever seen</td>\n",
       "      <td>1190</td>\n",
       "      <td>[health, global issues]</td>\n",
       "      <td>You've never seen data presented like this. Wi...</td>\n",
       "      <td>About 10 years ago, I took on the task to teac...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Simplicity sells</td>\n",
       "      <td>1286</td>\n",
       "      <td>[technology]</td>\n",
       "      <td>New York Times columnist David Pogue takes aim...</td>\n",
       "      <td>(Music: \"The Sound of Silence,\" Simon &amp; Garfun...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Greening the ghetto</td>\n",
       "      <td>1116</td>\n",
       "      <td>[business]</td>\n",
       "      <td>In an emotionally charged talk, MacArthur-winn...</td>\n",
       "      <td>If you're here today — and I'm very happy that...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Do schools kill creativity?</td>\n",
       "      <td>1164</td>\n",
       "      <td>[culture]</td>\n",
       "      <td>Sir Ken Robinson makes an entertaining and pro...</td>\n",
       "      <td>Good morning. How are you? (Audience) Good. It...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3396</th>\n",
       "      <td>4000</td>\n",
       "      <td>Crisis support for the world, one text away</td>\n",
       "      <td>690</td>\n",
       "      <td>[technology]</td>\n",
       "      <td>What if we could help people in crisis anytime...</td>\n",
       "      <td>\"I'm 14, and I want to go home.\" \"My name is B...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3397</th>\n",
       "      <td>4001</td>\n",
       "      <td>The dark history of IQ tests</td>\n",
       "      <td>346</td>\n",
       "      <td>[animation, history]</td>\n",
       "      <td>In 1905, psychologists Alfred Binet and Théodo...</td>\n",
       "      <td>In 1905, psychologists Alfred Binet and Théodo...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3398</th>\n",
       "      <td>4002</td>\n",
       "      <td>How \"policing for profit\" undermines your rights</td>\n",
       "      <td>774</td>\n",
       "      <td>[society]</td>\n",
       "      <td>Many countries have an active, centuries-old l...</td>\n",
       "      <td>Picture yourself driving down the road tomorro...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3399</th>\n",
       "      <td>4003</td>\n",
       "      <td>The electrifying speeches of Sojourner Truth</td>\n",
       "      <td>257</td>\n",
       "      <td>[animation, history]</td>\n",
       "      <td>Isabella Baumfree was born into slavery in lat...</td>\n",
       "      <td>In early 1828, Sojourner Truth approached the ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3400</th>\n",
       "      <td>4004</td>\n",
       "      <td>The most important anus in the ocean</td>\n",
       "      <td>281</td>\n",
       "      <td>[animation, science]</td>\n",
       "      <td>Is it a fuzzy sock? An overripe banana? A mold...</td>\n",
       "      <td>Can you guess what you’re looking at? Is it a ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3401 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index                                             title  duration  \\\n",
       "0         0                       Averting the climate crisis       977   \n",
       "1         1                   The best stats you've ever seen      1190   \n",
       "2         2                                  Simplicity sells      1286   \n",
       "3         3                               Greening the ghetto      1116   \n",
       "4         4                       Do schools kill creativity?      1164   \n",
       "...     ...                                               ...       ...   \n",
       "3396   4000       Crisis support for the world, one text away       690   \n",
       "3397   4001                      The dark history of IQ tests       346   \n",
       "3398   4002  How \"policing for profit\" undermines your rights       774   \n",
       "3399   4003      The electrifying speeches of Sojourner Truth       257   \n",
       "3400   4004              The most important anus in the ocean       281   \n",
       "\n",
       "                                             topics  \\\n",
       "0     [science, technology, global issues, culture]   \n",
       "1                           [health, global issues]   \n",
       "2                                      [technology]   \n",
       "3                                        [business]   \n",
       "4                                         [culture]   \n",
       "...                                             ...   \n",
       "3396                                   [technology]   \n",
       "3397                           [animation, history]   \n",
       "3398                                      [society]   \n",
       "3399                           [animation, history]   \n",
       "3400                           [animation, science]   \n",
       "\n",
       "                                            description  \\\n",
       "0     With the same humor and humanity he exuded in ...   \n",
       "1     You've never seen data presented like this. Wi...   \n",
       "2     New York Times columnist David Pogue takes aim...   \n",
       "3     In an emotionally charged talk, MacArthur-winn...   \n",
       "4     Sir Ken Robinson makes an entertaining and pro...   \n",
       "...                                                 ...   \n",
       "3396  What if we could help people in crisis anytime...   \n",
       "3397  In 1905, psychologists Alfred Binet and Théodo...   \n",
       "3398  Many countries have an active, centuries-old l...   \n",
       "3399  Isabella Baumfree was born into slavery in lat...   \n",
       "3400  Is it a fuzzy sock? An overripe banana? A mold...   \n",
       "\n",
       "                                             transcript  science  technology  \\\n",
       "0     Thank you so much, Chris. And it's truly a gre...      1.0         1.0   \n",
       "1     About 10 years ago, I took on the task to teac...      0.0         0.0   \n",
       "2     (Music: \"The Sound of Silence,\" Simon & Garfun...      0.0         1.0   \n",
       "3     If you're here today — and I'm very happy that...      0.0         0.0   \n",
       "4     Good morning. How are you? (Audience) Good. It...      0.0         0.0   \n",
       "...                                                 ...      ...         ...   \n",
       "3396  \"I'm 14, and I want to go home.\" \"My name is B...      0.0         1.0   \n",
       "3397  In 1905, psychologists Alfred Binet and Théodo...      0.0         0.0   \n",
       "3398  Picture yourself driving down the road tomorro...      0.0         0.0   \n",
       "3399  In early 1828, Sojourner Truth approached the ...      0.0         0.0   \n",
       "3400  Can you guess what you’re looking at? Is it a ...      1.0         0.0   \n",
       "\n",
       "      global issues  culture  health  business  design  social change  \\\n",
       "0               1.0      1.0     0.0       0.0     0.0            0.0   \n",
       "1               1.0      0.0     1.0       0.0     0.0            0.0   \n",
       "2               0.0      0.0     0.0       0.0     0.0            0.0   \n",
       "3               0.0      0.0     0.0       1.0     0.0            0.0   \n",
       "4               0.0      1.0     0.0       0.0     0.0            0.0   \n",
       "...             ...      ...     ...       ...     ...            ...   \n",
       "3396            0.0      0.0     0.0       0.0     0.0            0.0   \n",
       "3397            0.0      0.0     0.0       0.0     0.0            0.0   \n",
       "3398            0.0      0.0     0.0       0.0     0.0            0.0   \n",
       "3399            0.0      0.0     0.0       0.0     0.0            0.0   \n",
       "3400            0.0      0.0     0.0       0.0     0.0            0.0   \n",
       "\n",
       "      history  animation  society  \n",
       "0         0.0        0.0      0.0  \n",
       "1         0.0        0.0      0.0  \n",
       "2         0.0        0.0      0.0  \n",
       "3         0.0        0.0      0.0  \n",
       "4         0.0        0.0      0.0  \n",
       "...       ...        ...      ...  \n",
       "3396      0.0        0.0      0.0  \n",
       "3397      1.0        1.0      0.0  \n",
       "3398      0.0        0.0      1.0  \n",
       "3399      1.0        1.0      0.0  \n",
       "3400      0.0        1.0      0.0  \n",
       "\n",
       "[3401 rows x 17 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Tokenizacja i rdzeniowanie metodami Snowball i Porter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1108)>\n"
     ]
    }
   ],
   "source": [
    "# Snowwball stemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "snow = SnowballStemmer(language='english')\n",
    "\n",
    "# Porter stemmer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "\n",
    "# Stop words\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trans_trim(text,stemmer):\n",
    "    def text_trimming(word):\n",
    "        replace_marks = [\".\",\",\",\"?\",\"-\",\":\",\"[\",\"]\",\"!\",\")\",\"(\",\"\\\\\"]\n",
    "        for mark in replace_marks:\n",
    "            word  = word.replace(mark,\"\")\n",
    "        word = word.lower()\n",
    "        word = stemmer.stem(word)\n",
    "        return word\n",
    "    temp = [word for word in text.split()]\n",
    "    transcript = [text_trimming(word) for word in temp if word not in stop and len(word) > 1]\n",
    "    return transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens(list_of_txts,stemmer):\n",
    "    tokens_arr = [trans_trim(text,stemmer) for text in tqdm(list_of_txts)]\n",
    "    return tokens_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 3401/3401 [01:21<00:00, 41.54it/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_tr_s = get_tokens(df[\"transcript\"],snow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 3401/3401 [01:55<00:00, 29.33it/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_tr_p = get_tokens(df[\"transcript\"],porter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redukcja rzadich wyrazow\n",
    "def rare_reduction(tokenized_text,size = 10000):\n",
    "    vocab = defaultdict(int)\n",
    "    length = range(sum([len(transcr) for transcr in tokenized_text]))\n",
    "\n",
    "    for transcript in tokenized_tr_s:\n",
    "        for word in transcript:\n",
    "            vocab[word] += 1\n",
    "    vocab = pd.Series(vocab).sort_values(ascending = False).head(size)\n",
    "    for transcript in tqdm(tokenized_text):\n",
    "        for word in transcript:\n",
    "            if word not in vocab:\n",
    "                transcript.remove(word)\n",
    "    return tokenized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 3401/3401 [00:05<00:00, 638.27it/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_tr_s = rare_reduction(tokenized_tr_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 3401/3401 [00:06<00:00, 551.34it/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_tr_p = rare_reduction(tokenized_tr_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_again(texts):\n",
    "    joined_texts = []\n",
    "    for text in texts:\n",
    "        joined_texts.append(\" \".join(text))\n",
    "    return joined_texts\n",
    "\n",
    "transc_joined = join_again(tokenized_tr_s)\n",
    "transc_joined_p = join_again(tokenized_tr_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Thank you so much, Chris. And it's truly a great honor to have the opportunity t\""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[0, \"transcript\"][:80]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'thank much chris and truli great honor opportun co'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transc_joined[0][:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'thank much and truli great honor opportun come sta'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transc_joined_p[0][:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Podział zbioru i TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y = df.drop([\"title\", \"duration\", \"topics\", \"description\", \"transcript\", \"index\"], axis = 1)\n",
    "\n",
    "def split_data(transc_joined, y):\n",
    "    return train_test_split(transc_joined, y, test_size=0.2, random_state=1)\n",
    "train_text, test_text, y_train, y_test = split_data(transc_joined, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(analyzer='word', norm='l2', ngram_range=(1,1)) #  ngram_range=(1,3),strip_accents='unicode',\n",
    "vectorizer.fit(train_text)\n",
    "vectorizer.fit(test_text)\n",
    "x_train = vectorizer.transform(train_text)\n",
    "x_test = vectorizer.transform(test_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Klasyfikacja multi-label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chain classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =  0.19236417033773862\n"
     ]
    }
   ],
   "source": [
    "# Chain classifier\n",
    "def chain_classifier(x_train,y_train,x_test, y_test):\n",
    "    from skmultilearn.problem_transform import ClassifierChain\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    # initialize classifier chains multi-label classifier\n",
    "    classifier = ClassifierChain(LogisticRegression())\n",
    "    # Training logistic regression model on train data\n",
    "    classifier.fit(x_train, y_train)\n",
    "    # predict\n",
    "    predictions = classifier.predict(x_test)\n",
    "    # accuracy\n",
    "    print(\"Accuracy = \",accuracy_score(y_test,predictions))\n",
    "    return predictions\n",
    "\n",
    "chain_preds = chain_classifier(x_train,y_train,x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LabelPowerset classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =  0.2511013215859031\n"
     ]
    }
   ],
   "source": [
    "# using Label Powerset\n",
    "def powerset_classifier(x_train,y_train,x_test, y_test):\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from skmultilearn.problem_transform import LabelPowerset\n",
    "    # initialize label powerset multi-label classifier\n",
    "    classifier = LabelPowerset(LogisticRegression())\n",
    "    # train\n",
    "    classifier.fit(x_train, y_train)\n",
    "    # predict\n",
    "    predictions = classifier.predict(x_test)\n",
    "    # accuracy\n",
    "    print(\"Accuracy = \",accuracy_score(y_test,predictions))\n",
    "    return predictions\n",
    "\n",
    "powerset_preds = powerset_classifier(x_train,y_train,x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BinaryRelevance classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =  0.06754772393538913\n"
     ]
    }
   ],
   "source": [
    "def brelevance_classifier(x_train,y_train,x_test,y_test):\n",
    "    from skmultilearn.problem_transform import BinaryRelevance\n",
    "    from sklearn.naive_bayes import GaussianNB\n",
    "    classifier = BinaryRelevance(GaussianNB())  \n",
    "    classifier.fit(x_train, y_train)  \n",
    "    predictions = classifier.predict(x_test)\n",
    "    # accuracy\n",
    "    print(\"Accuracy = \",accuracy_score(y_test,predictions))\n",
    "    return predictions\n",
    "\n",
    "brelevance_preds = brelevance_classifier(x_train,y_train,x_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Wyniki dla metody porter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text, test_text, y_train, y_test = split_data(transc_joined_p, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =  0.19236417033773862\n"
     ]
    }
   ],
   "source": [
    "chain_preds = chain_classifier(x_train,y_train,x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =  0.2511013215859031\n"
     ]
    }
   ],
   "source": [
    "powerset_preds_p = powerset_classifier(x_train,y_train,x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =  0.06754772393538913\n"
     ]
    }
   ],
   "source": [
    "brelevance_preds = brelevance_classifier(x_train,y_train,x_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Wyniki dla poszczególnych klas uzyskane metodą Label Powerset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "      science       0.70      0.80      0.75       200\n",
      "   technology       0.79      0.53      0.63       195\n",
      "      culture       0.66      0.43      0.52       119\n",
      "global issues       0.51      0.30      0.38       144\n",
      "      society       0.93      0.14      0.25        98\n",
      "       design       0.84      0.39      0.53        82\n",
      "social change       0.81      0.37      0.51       102\n",
      "    animation       0.72      0.47      0.57       106\n",
      "     business       1.00      0.13      0.23        77\n",
      "       health       0.87      0.49      0.62       107\n",
      "      history       0.54      0.34      0.42       109\n",
      "\n",
      "    micro avg       0.71      0.44      0.54      1339\n",
      "    macro avg       0.76      0.40      0.49      1339\n",
      " weighted avg       0.74      0.44      0.52      1339\n",
      "  samples avg       0.71      0.49      0.55      1339\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "target_names = cat_series.keys()\n",
    "print(classification_report(y_test, powerset_preds, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jakub\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\jakub\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "report_power = classification_report(y_test, powerset_preds, target_names=target_names, output_dict=True)\n",
    "report_chain = classification_report(y_test, chain_preds, target_names=target_names, output_dict=True)\n",
    "report_brelevance = classification_report(y_test, brelevance_preds, target_names=target_names, output_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories_names = [key for key in report_power.keys()][:num_classes]\n",
    "f1_power = np.array([report_power[cat][\"f1-score\"] for cat in categories_names]).round(2)\n",
    "f1_chain = np.array([report_chain[cat][\"f1-score\"] for cat in categories_names]).round(2)\n",
    "f1_brelevance = np.array([report_brelevance[cat][\"f1-score\"] for cat in categories_names]).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Model bez uwzględnienia wag klas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_words = max([len(i) for i in tokenized_tr_s])\n",
    "max_words = 500\n",
    "maxlen = 1300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topics_bin = df.drop([\"title\", \"duration\", \"topics\", \"description\", \"transcript\", \"index\"], axis = 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=max_words, lower=True)\n",
    "tokenizer.fit_on_texts(transc_joined) # transc_joined\n",
    "\n",
    "def get_features(text_series, maxlen = 1300):\n",
    "    sequences = tokenizer.texts_to_sequences(text_series)\n",
    "    return pad_sequences(sequences, maxlen=maxlen)\n",
    "x = get_features(transc_joined) # transc_joined\n",
    "y = np.array(df_topics_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.1, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Embedding, Flatten, GlobalMaxPool1D, Dropout, Conv1D, Input, BatchNormalization\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.losses import binary_crossentropy\n",
    "from tensorflow.keras.optimizers import Adam, Adagrad\n",
    "def keras_seq(class_weight):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim =  max_words, output_dim = 512, input_length=maxlen))\n",
    "    model.add(Conv1D(512, 12, padding='valid', activation='sigmoid', strides=1))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(GlobalMaxPool1D())\n",
    "\n",
    "    model.add(Dense(num_classes))\n",
    "#     model.add(BatchNormalization())\n",
    "    model.add(Activation('sigmoid'))\n",
    "\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['categorical_accuracy'])\n",
    "    model.summary()\n",
    "\n",
    "    callbacks = [\n",
    "        ReduceLROnPlateau(), \n",
    "        EarlyStopping(patience=4), \n",
    "        ModelCheckpoint(filepath='model-conv1d.h5', save_best_only=True)\n",
    "    ]\n",
    "    \n",
    "    history = model.fit(x_train, y_train,\n",
    "                        class_weight=class_weight,\n",
    "                        epochs=45,\n",
    "                        batch_size=256,\n",
    "                        validation_split=0.1,\n",
    "                        callbacks=callbacks)\n",
    "    return history, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 1300, 512)         256000    \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 1289, 512)         3146240   \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1289, 512)         0         \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 11)                5643      \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 11)                0         \n",
      "=================================================================\n",
      "Total params: 3,407,883\n",
      "Trainable params: 3,407,883\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/45\n",
      "11/11 [==============================] - 37s 2s/step - loss: 0.5628 - categorical_accuracy: 0.2607 - val_loss: 0.4669 - val_categorical_accuracy: 0.1634\n",
      "Epoch 2/45\n",
      "11/11 [==============================] - 13s 1s/step - loss: 0.4685 - categorical_accuracy: 0.2298 - val_loss: 0.4674 - val_categorical_accuracy: 0.2810\n",
      "Epoch 3/45\n",
      "11/11 [==============================] - 13s 1s/step - loss: 0.4483 - categorical_accuracy: 0.3188 - val_loss: 0.4578 - val_categorical_accuracy: 0.2778\n",
      "Epoch 4/45\n",
      "11/11 [==============================] - 13s 1s/step - loss: 0.4366 - categorical_accuracy: 0.3417 - val_loss: 0.4479 - val_categorical_accuracy: 0.3072\n",
      "Epoch 5/45\n",
      "11/11 [==============================] - 13s 1s/step - loss: 0.4204 - categorical_accuracy: 0.3747 - val_loss: 0.4333 - val_categorical_accuracy: 0.4020\n",
      "Epoch 6/45\n",
      "11/11 [==============================] - 13s 1s/step - loss: 0.3987 - categorical_accuracy: 0.4067 - val_loss: 0.4134 - val_categorical_accuracy: 0.4346\n",
      "Epoch 7/45\n",
      "11/11 [==============================] - 13s 1s/step - loss: 0.3770 - categorical_accuracy: 0.4328 - val_loss: 0.3964 - val_categorical_accuracy: 0.4314\n",
      "Epoch 8/45\n",
      "11/11 [==============================] - 13s 1s/step - loss: 0.3570 - categorical_accuracy: 0.4706 - val_loss: 0.3851 - val_categorical_accuracy: 0.4379\n",
      "Epoch 9/45\n",
      "11/11 [==============================] - 13s 1s/step - loss: 0.3398 - categorical_accuracy: 0.4935 - val_loss: 0.3731 - val_categorical_accuracy: 0.4379\n",
      "Epoch 10/45\n",
      "11/11 [==============================] - 13s 1s/step - loss: 0.3250 - categorical_accuracy: 0.4938 - val_loss: 0.3637 - val_categorical_accuracy: 0.4477\n",
      "Epoch 11/45\n",
      "11/11 [==============================] - 13s 1s/step - loss: 0.3116 - categorical_accuracy: 0.5142 - val_loss: 0.3528 - val_categorical_accuracy: 0.4444\n",
      "Epoch 12/45\n",
      "11/11 [==============================] - 13s 1s/step - loss: 0.2990 - categorical_accuracy: 0.5218 - val_loss: 0.3482 - val_categorical_accuracy: 0.4706\n",
      "Epoch 13/45\n",
      "11/11 [==============================] - 13s 1s/step - loss: 0.2869 - categorical_accuracy: 0.5352 - val_loss: 0.3430 - val_categorical_accuracy: 0.4477\n",
      "Epoch 14/45\n",
      "11/11 [==============================] - 13s 1s/step - loss: 0.2773 - categorical_accuracy: 0.5472 - val_loss: 0.3386 - val_categorical_accuracy: 0.4183\n",
      "Epoch 15/45\n",
      "11/11 [==============================] - 13s 1s/step - loss: 0.2657 - categorical_accuracy: 0.5563 - val_loss: 0.3379 - val_categorical_accuracy: 0.4477\n",
      "Epoch 16/45\n",
      "11/11 [==============================] - 13s 1s/step - loss: 0.2553 - categorical_accuracy: 0.5752 - val_loss: 0.3309 - val_categorical_accuracy: 0.4837\n",
      "Epoch 17/45\n",
      "11/11 [==============================] - 13s 1s/step - loss: 0.2440 - categorical_accuracy: 0.5948 - val_loss: 0.3300 - val_categorical_accuracy: 0.4477\n",
      "Epoch 18/45\n",
      "11/11 [==============================] - 13s 1s/step - loss: 0.2337 - categorical_accuracy: 0.5969 - val_loss: 0.3262 - val_categorical_accuracy: 0.4869\n",
      "Epoch 19/45\n",
      "11/11 [==============================] - 13s 1s/step - loss: 0.2231 - categorical_accuracy: 0.6053 - val_loss: 0.3232 - val_categorical_accuracy: 0.4641\n",
      "Epoch 20/45\n",
      "11/11 [==============================] - 13s 1s/step - loss: 0.2132 - categorical_accuracy: 0.6180 - val_loss: 0.3218 - val_categorical_accuracy: 0.4412\n",
      "Epoch 21/45\n",
      "11/11 [==============================] - 13s 1s/step - loss: 0.2021 - categorical_accuracy: 0.6362 - val_loss: 0.3177 - val_categorical_accuracy: 0.4706\n",
      "Epoch 22/45\n",
      "11/11 [==============================] - 13s 1s/step - loss: 0.1910 - categorical_accuracy: 0.6264 - val_loss: 0.3168 - val_categorical_accuracy: 0.4935\n",
      "Epoch 23/45\n",
      "11/11 [==============================] - 13s 1s/step - loss: 0.1804 - categorical_accuracy: 0.6518 - val_loss: 0.3166 - val_categorical_accuracy: 0.4542\n",
      "Epoch 24/45\n",
      "11/11 [==============================] - 13s 1s/step - loss: 0.1708 - categorical_accuracy: 0.6460 - val_loss: 0.3160 - val_categorical_accuracy: 0.4575\n",
      "Epoch 25/45\n",
      "11/11 [==============================] - 13s 1s/step - loss: 0.1604 - categorical_accuracy: 0.6590 - val_loss: 0.3141 - val_categorical_accuracy: 0.4412\n",
      "Epoch 26/45\n",
      "11/11 [==============================] - 13s 1s/step - loss: 0.1501 - categorical_accuracy: 0.6739 - val_loss: 0.3128 - val_categorical_accuracy: 0.4379\n",
      "Epoch 27/45\n",
      "11/11 [==============================] - 14s 1s/step - loss: 0.1401 - categorical_accuracy: 0.6692 - val_loss: 0.3132 - val_categorical_accuracy: 0.4412\n",
      "Epoch 28/45\n",
      "11/11 [==============================] - 14s 1s/step - loss: 0.1307 - categorical_accuracy: 0.6641 - val_loss: 0.3118 - val_categorical_accuracy: 0.4542\n",
      "Epoch 29/45\n",
      "11/11 [==============================] - 14s 1s/step - loss: 0.1219 - categorical_accuracy: 0.6652 - val_loss: 0.3111 - val_categorical_accuracy: 0.4673\n",
      "Epoch 30/45\n",
      "11/11 [==============================] - 14s 1s/step - loss: 0.1130 - categorical_accuracy: 0.6790 - val_loss: 0.3099 - val_categorical_accuracy: 0.4477\n",
      "Epoch 31/45\n",
      "11/11 [==============================] - 14s 1s/step - loss: 0.1046 - categorical_accuracy: 0.6888 - val_loss: 0.3103 - val_categorical_accuracy: 0.4510\n",
      "Epoch 32/45\n",
      "11/11 [==============================] - 14s 1s/step - loss: 0.0970 - categorical_accuracy: 0.6830 - val_loss: 0.3100 - val_categorical_accuracy: 0.4542\n",
      "Epoch 33/45\n",
      "11/11 [==============================] - 14s 1s/step - loss: 0.0893 - categorical_accuracy: 0.6805 - val_loss: 0.3104 - val_categorical_accuracy: 0.4739\n",
      "Epoch 34/45\n",
      "11/11 [==============================] - 14s 1s/step - loss: 0.0830 - categorical_accuracy: 0.6779 - val_loss: 0.3095 - val_categorical_accuracy: 0.4444\n",
      "Epoch 35/45\n",
      "11/11 [==============================] - 14s 1s/step - loss: 0.0764 - categorical_accuracy: 0.6819 - val_loss: 0.3101 - val_categorical_accuracy: 0.4575\n",
      "Epoch 36/45\n",
      "11/11 [==============================] - 14s 1s/step - loss: 0.0711 - categorical_accuracy: 0.6841 - val_loss: 0.3110 - val_categorical_accuracy: 0.4510\n",
      "Epoch 37/45\n",
      "11/11 [==============================] - 14s 1s/step - loss: 0.0657 - categorical_accuracy: 0.6805 - val_loss: 0.3113 - val_categorical_accuracy: 0.4477\n",
      "Epoch 38/45\n",
      "11/11 [==============================] - 15s 1s/step - loss: 0.0610 - categorical_accuracy: 0.6852 - val_loss: 0.3119 - val_categorical_accuracy: 0.4412\n"
     ]
    }
   ],
   "source": [
    "history, model = keras_seq(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model = keras.models.load_model('model-conv1d.h5')\n",
    "predictions = cnn_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def prob_to_class(predictions):\n",
    "    classes_predictions = []\n",
    "    for arr in predictions:\n",
    "        classes_predictions.append([1 if i > 0.5 else 0 for i in arr])\n",
    "    return classes_predictions\n",
    "\n",
    "classes_predictions = prob_to_class(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>animation_pred</th>\n",
       "      <th>animation_true</th>\n",
       "      <th>business_pred</th>\n",
       "      <th>business_true</th>\n",
       "      <th>culture_pred</th>\n",
       "      <th>culture_true</th>\n",
       "      <th>design_pred</th>\n",
       "      <th>design_true</th>\n",
       "      <th>global issues_pred</th>\n",
       "      <th>global issues_true</th>\n",
       "      <th>...</th>\n",
       "      <th>history_pred</th>\n",
       "      <th>history_true</th>\n",
       "      <th>science_pred</th>\n",
       "      <th>science_true</th>\n",
       "      <th>social change_pred</th>\n",
       "      <th>social change_true</th>\n",
       "      <th>society_pred</th>\n",
       "      <th>society_true</th>\n",
       "      <th>technology_pred</th>\n",
       "      <th>technology_true</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>331</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>334</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>336</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>338</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     animation_pred  animation_true  business_pred  business_true  \\\n",
       "331               0             0.0              0            0.0   \n",
       "332               1             1.0              0            0.0   \n",
       "333               0             0.0              0            0.0   \n",
       "334               0             0.0              0            0.0   \n",
       "335               0             0.0              0            0.0   \n",
       "336               0             0.0              0            0.0   \n",
       "337               0             0.0              0            0.0   \n",
       "338               0             0.0              0            1.0   \n",
       "339               0             0.0              0            0.0   \n",
       "340               0             0.0              0            0.0   \n",
       "\n",
       "     culture_pred  culture_true  design_pred  design_true  global issues_pred  \\\n",
       "331             0           0.0            0          1.0                   0   \n",
       "332             0           1.0            0          0.0                   0   \n",
       "333             0           0.0            0          0.0                   0   \n",
       "334             0           0.0            0          0.0                   1   \n",
       "335             0           0.0            0          0.0                   0   \n",
       "336             0           0.0            0          0.0                   1   \n",
       "337             0           0.0            0          0.0                   0   \n",
       "338             0           1.0            0          0.0                   0   \n",
       "339             0           1.0            0          0.0                   0   \n",
       "340             0           0.0            0          0.0                   0   \n",
       "\n",
       "     global issues_true  ...  history_pred  history_true  science_pred  \\\n",
       "331                 0.0  ...             0           0.0             0   \n",
       "332                 0.0  ...             1           1.0             0   \n",
       "333                 0.0  ...             0           0.0             0   \n",
       "334                 1.0  ...             0           0.0             0   \n",
       "335                 0.0  ...             0           0.0             0   \n",
       "336                 1.0  ...             0           0.0             0   \n",
       "337                 0.0  ...             0           0.0             1   \n",
       "338                 1.0  ...             0           1.0             0   \n",
       "339                 0.0  ...             0           0.0             0   \n",
       "340                 1.0  ...             0           0.0             0   \n",
       "\n",
       "     science_true  social change_pred  social change_true  society_pred  \\\n",
       "331           0.0                   0                 0.0             0   \n",
       "332           0.0                   0                 0.0             0   \n",
       "333           0.0                   1                 1.0             1   \n",
       "334           0.0                   0                 0.0             0   \n",
       "335           0.0                   0                 1.0             0   \n",
       "336           0.0                   1                 0.0             1   \n",
       "337           0.0                   0                 0.0             0   \n",
       "338           0.0                   0                 0.0             0   \n",
       "339           0.0                   0                 0.0             0   \n",
       "340           0.0                   0                 0.0             0   \n",
       "\n",
       "     society_true  technology_pred  technology_true  \n",
       "331           0.0                1              1.0  \n",
       "332           0.0                0              0.0  \n",
       "333           1.0                0              0.0  \n",
       "334           0.0                0              0.0  \n",
       "335           1.0                0              0.0  \n",
       "336           0.0                0              0.0  \n",
       "337           0.0                1              0.0  \n",
       "338           0.0                0              0.0  \n",
       "339           0.0                0              0.0  \n",
       "340           0.0                0              0.0  \n",
       "\n",
       "[10 rows x 22 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns_pred = [f\"{col}_pred\" for col in df_topics_bin.columns]\n",
    "pred_df = pd.DataFrame(classes_predictions, columns =  columns_pred)\n",
    "\n",
    "columns_true = [f\"{col}_true\" for col in df_topics_bin.columns]\n",
    "true_df = pd.DataFrame(y_test, columns =  columns_true)\n",
    "\n",
    "scoring_df = pd.concat([pred_df,true_df], axis = 1)\n",
    "scoring_df = scoring_df.reindex(sorted(scoring_df.columns), axis=1)\n",
    "scoring_df.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 0s 31ms/step - loss: 0.3024 - categorical_accuracy: 0.4927\n",
      "categorical_accuracy: 0.49266862869262695\n"
     ]
    }
   ],
   "source": [
    "cnn_model = keras.models.load_model('model-conv1d.h5')\n",
    "metrics = cnn_model.evaluate(x_test, y_test)\n",
    "print(\"{}: {}\".format(model.metrics_names[1], metrics[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "      science       0.69      0.70      0.70       101\n",
      "   technology       0.74      0.55      0.63       109\n",
      "      culture       0.56      0.31      0.40        65\n",
      "global issues       0.48      0.15      0.23        66\n",
      "      society       0.64      0.44      0.52        48\n",
      "       design       0.65      0.43      0.52        35\n",
      "social change       0.70      0.42      0.53        50\n",
      "    animation       0.44      0.32      0.37        38\n",
      "     business       0.73      0.34      0.47        32\n",
      "       health       0.94      0.96      0.95        53\n",
      "      history       0.37      0.21      0.27        48\n",
      "\n",
      "    micro avg       0.67      0.47      0.55       645\n",
      "    macro avg       0.63      0.44      0.51       645\n",
      " weighted avg       0.64      0.47      0.53       645\n",
      "  samples avg       0.55      0.50      0.49       645\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jakub\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "target_names = cat_series.keys()\n",
    "print(classification_report(y_test, classes_predictions, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jakub\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "keras_one = classification_report(y_test, classes_predictions, target_names=target_names, output_dict=True)\n",
    "f1_keras_one = np.array([keras_one[cat][\"f1-score\"] for cat in categories_names]).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Model uwzględniający wagi klas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 1300, 512)         256000    \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 1289, 512)         3146240   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1289, 512)         0         \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 11)                5643      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 11)                0         \n",
      "=================================================================\n",
      "Total params: 3,407,883\n",
      "Trainable params: 3,407,883\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/45\n",
      "11/11 [==============================] - 15s 1s/step - loss: 0.2988 - categorical_accuracy: 0.1997 - val_loss: 0.4674 - val_categorical_accuracy: 0.2843\n",
      "Epoch 2/45\n",
      "11/11 [==============================] - 14s 1s/step - loss: 0.2698 - categorical_accuracy: 0.2469 - val_loss: 0.4681 - val_categorical_accuracy: 0.1373\n",
      "Epoch 3/45\n",
      "11/11 [==============================] - 14s 1s/step - loss: 0.2600 - categorical_accuracy: 0.1699 - val_loss: 0.4603 - val_categorical_accuracy: 0.1928\n",
      "Epoch 4/45\n",
      "11/11 [==============================] - 14s 1s/step - loss: 0.2533 - categorical_accuracy: 0.3003 - val_loss: 0.4480 - val_categorical_accuracy: 0.2255\n",
      "Epoch 5/45\n",
      "11/11 [==============================] - 14s 1s/step - loss: 0.2434 - categorical_accuracy: 0.3805 - val_loss: 0.4345 - val_categorical_accuracy: 0.3170\n",
      "Epoch 6/45\n",
      "11/11 [==============================] - 14s 1s/step - loss: 0.2294 - categorical_accuracy: 0.4325 - val_loss: 0.4107 - val_categorical_accuracy: 0.3954\n",
      "Epoch 7/45\n",
      "11/11 [==============================] - 14s 1s/step - loss: 0.2152 - categorical_accuracy: 0.4426 - val_loss: 0.3920 - val_categorical_accuracy: 0.4118\n",
      "Epoch 8/45\n",
      "11/11 [==============================] - 14s 1s/step - loss: 0.2023 - categorical_accuracy: 0.4521 - val_loss: 0.3801 - val_categorical_accuracy: 0.3791\n",
      "Epoch 9/45\n",
      "11/11 [==============================] - 14s 1s/step - loss: 0.1917 - categorical_accuracy: 0.4710 - val_loss: 0.3641 - val_categorical_accuracy: 0.4118\n",
      "Epoch 10/45\n",
      "11/11 [==============================] - 14s 1s/step - loss: 0.1825 - categorical_accuracy: 0.4884 - val_loss: 0.3568 - val_categorical_accuracy: 0.4118\n",
      "Epoch 11/45\n",
      "11/11 [==============================] - 14s 1s/step - loss: 0.1748 - categorical_accuracy: 0.4855 - val_loss: 0.3476 - val_categorical_accuracy: 0.4118\n",
      "Epoch 12/45\n",
      "11/11 [==============================] - 14s 1s/step - loss: 0.1667 - categorical_accuracy: 0.5087 - val_loss: 0.3428 - val_categorical_accuracy: 0.4379\n",
      "Epoch 13/45\n",
      "11/11 [==============================] - 14s 1s/step - loss: 0.1593 - categorical_accuracy: 0.5127 - val_loss: 0.3358 - val_categorical_accuracy: 0.4641\n",
      "Epoch 14/45\n",
      "11/11 [==============================] - 14s 1s/step - loss: 0.1527 - categorical_accuracy: 0.5276 - val_loss: 0.3301 - val_categorical_accuracy: 0.4281\n",
      "Epoch 15/45\n",
      "11/11 [==============================] - 14s 1s/step - loss: 0.1456 - categorical_accuracy: 0.5432 - val_loss: 0.3293 - val_categorical_accuracy: 0.4510\n",
      "Epoch 16/45\n",
      "11/11 [==============================] - 14s 1s/step - loss: 0.1388 - categorical_accuracy: 0.5690 - val_loss: 0.3242 - val_categorical_accuracy: 0.4673\n",
      "Epoch 17/45\n",
      "11/11 [==============================] - 14s 1s/step - loss: 0.1320 - categorical_accuracy: 0.5886 - val_loss: 0.3233 - val_categorical_accuracy: 0.4641\n",
      "Epoch 18/45\n",
      "11/11 [==============================] - 14s 1s/step - loss: 0.1252 - categorical_accuracy: 0.5904 - val_loss: 0.3190 - val_categorical_accuracy: 0.4739\n",
      "Epoch 19/45\n",
      "11/11 [==============================] - 14s 1s/step - loss: 0.1184 - categorical_accuracy: 0.5999 - val_loss: 0.3176 - val_categorical_accuracy: 0.4542\n",
      "Epoch 20/45\n",
      "11/11 [==============================] - 14s 1s/step - loss: 0.1121 - categorical_accuracy: 0.6173 - val_loss: 0.3165 - val_categorical_accuracy: 0.4477\n",
      "Epoch 21/45\n",
      "11/11 [==============================] - 15s 1s/step - loss: 0.1051 - categorical_accuracy: 0.6369 - val_loss: 0.3150 - val_categorical_accuracy: 0.4477\n",
      "Epoch 22/45\n",
      "11/11 [==============================] - 14s 1s/step - loss: 0.0988 - categorical_accuracy: 0.6271 - val_loss: 0.3115 - val_categorical_accuracy: 0.4575\n",
      "Epoch 23/45\n",
      "11/11 [==============================] - 14s 1s/step - loss: 0.0926 - categorical_accuracy: 0.6420 - val_loss: 0.3119 - val_categorical_accuracy: 0.4379\n",
      "Epoch 24/45\n",
      "11/11 [==============================] - 14s 1s/step - loss: 0.0871 - categorical_accuracy: 0.6449 - val_loss: 0.3112 - val_categorical_accuracy: 0.4510\n",
      "Epoch 25/45\n",
      "11/11 [==============================] - 14s 1s/step - loss: 0.0814 - categorical_accuracy: 0.6532 - val_loss: 0.3099 - val_categorical_accuracy: 0.4477\n",
      "Epoch 26/45\n",
      "11/11 [==============================] - 14s 1s/step - loss: 0.0753 - categorical_accuracy: 0.6728 - val_loss: 0.3090 - val_categorical_accuracy: 0.4444\n",
      "Epoch 27/45\n",
      "11/11 [==============================] - 14s 1s/step - loss: 0.0699 - categorical_accuracy: 0.6630 - val_loss: 0.3093 - val_categorical_accuracy: 0.4477\n",
      "Epoch 28/45\n",
      "11/11 [==============================] - 14s 1s/step - loss: 0.0650 - categorical_accuracy: 0.6692 - val_loss: 0.3070 - val_categorical_accuracy: 0.4510\n",
      "Epoch 29/45\n",
      "11/11 [==============================] - 14s 1s/step - loss: 0.0602 - categorical_accuracy: 0.6739 - val_loss: 0.3072 - val_categorical_accuracy: 0.4641\n",
      "Epoch 30/45\n",
      "11/11 [==============================] - 14s 1s/step - loss: 0.0556 - categorical_accuracy: 0.6794 - val_loss: 0.3066 - val_categorical_accuracy: 0.4575\n",
      "Epoch 31/45\n",
      "11/11 [==============================] - 14s 1s/step - loss: 0.0517 - categorical_accuracy: 0.6874 - val_loss: 0.3064 - val_categorical_accuracy: 0.4542\n",
      "Epoch 32/45\n",
      "11/11 [==============================] - 14s 1s/step - loss: 0.0477 - categorical_accuracy: 0.6874 - val_loss: 0.3073 - val_categorical_accuracy: 0.4477\n",
      "Epoch 33/45\n",
      "11/11 [==============================] - 14s 1s/step - loss: 0.0441 - categorical_accuracy: 0.6895 - val_loss: 0.3074 - val_categorical_accuracy: 0.4771\n",
      "Epoch 34/45\n",
      "11/11 [==============================] - 14s 1s/step - loss: 0.0407 - categorical_accuracy: 0.6950 - val_loss: 0.3066 - val_categorical_accuracy: 0.4542\n",
      "Epoch 35/45\n",
      "11/11 [==============================] - 15s 1s/step - loss: 0.0376 - categorical_accuracy: 0.6892 - val_loss: 0.3069 - val_categorical_accuracy: 0.4641\n"
     ]
    }
   ],
   "source": [
    "history, model = keras_seq(class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 0s 30ms/step - loss: 0.3048 - categorical_accuracy: 0.4457\n",
      "categorical_accuracy: 0.4457477927207947\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "      science       0.69      0.60      0.65       101\n",
      "   technology       0.77      0.51      0.62       109\n",
      "      culture       0.57      0.35      0.44        65\n",
      "global issues       0.55      0.18      0.27        66\n",
      "      society       0.74      0.54      0.63        48\n",
      "       design       0.62      0.46      0.52        35\n",
      "social change       0.70      0.46      0.55        50\n",
      "    animation       0.38      0.29      0.33        38\n",
      "     business       0.73      0.34      0.47        32\n",
      "       health       0.94      0.94      0.94        53\n",
      "      history       0.38      0.19      0.25        48\n",
      "\n",
      "    micro avg       0.68      0.46      0.55       645\n",
      "    macro avg       0.64      0.44      0.52       645\n",
      " weighted avg       0.66      0.46      0.53       645\n",
      "  samples avg       0.55      0.49      0.49       645\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jakub\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "cnn_model = keras.models.load_model('model-conv1d.h5')\n",
    "metrics = cnn_model.evaluate(x_test, y_test)\n",
    "print(\"{}: {}\".format(model.metrics_names[1], metrics[1]))\n",
    "\n",
    "predictions = cnn_model.predict(x_test)\n",
    "classes_predictions = prob_to_class(predictions)\n",
    "print(classification_report(y_test, classes_predictions, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jakub\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "keras_one_w = classification_report(y_test, classes_predictions, target_names=target_names, output_dict=True)\n",
    "f1_keras_one_w = np.array([keras_one_w[cat][\"f1-score\"] for cat in categories_names]).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Funkcyjny model 3-wejściowy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_for_test = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2901/2901 [01:12<00:00, 39.81it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 2901/2901 [00:02<00:00, 1156.37it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 2901/2901 [00:00<00:00, 7180.58it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 500/500 [00:09<00:00, 53.60it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 500/500 [00:00<00:00, 904.17it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 500/500 [00:00<00:00, 8333.80it/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_transcr_s = get_tokens(df[\"transcript\"].iloc[:-rows_for_test],snow)\n",
    "tokenized_descr_s = get_tokens(df[\"description\"].iloc[:-rows_for_test],snow)\n",
    "tokenized_title_s = get_tokens(df[\"title\"].iloc[:-rows_for_test],snow)\n",
    "\n",
    "tokenized_transcr_test = get_tokens(df[\"transcript\"].iloc[-rows_for_test:],snow)\n",
    "tokenized_descr_test = get_tokens(df[\"description\"].iloc[-rows_for_test:],snow)\n",
    "tokenized_title_test = get_tokens(df[\"title\"].iloc[-rows_for_test:],snow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 2901/2901 [00:00<00:00, 22317.04it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 2901/2901 [00:00<00:00, 21976.84it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 2901/2901 [00:00<00:00, 145053.60it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 500/500 [00:00<00:00, 19231.63it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 500/500 [00:00<00:00, 20833.39it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 500/500 [00:00<00:00, 125016.51it/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_transcr_s = rare_reduction(tokenized_descr_s)\n",
    "tokenized_descr_s = rare_reduction(tokenized_descr_s)\n",
    "tokenized_title_s = rare_reduction(tokenized_title_s)\n",
    "\n",
    "tokenized_transcr_test = rare_reduction(tokenized_descr_test)\n",
    "tokenized_descr_test = rare_reduction(tokenized_descr_test)\n",
    "tokenized_title_test = rare_reduction(tokenized_title_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "transc_joined = join_again(tokenized_transcr_s)\n",
    "descr_joined = join_again(tokenized_descr_s)\n",
    "title_joined = join_again(tokenized_title_s)\n",
    "\n",
    "transc_joined_test = join_again(tokenized_transcr_test)\n",
    "descr_joined_test = join_again(tokenized_descr_test)\n",
    "title_joined_test = join_again(tokenized_title_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2901, 50), (2901, 1500), (2901, 11), (500, 11))"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_size_1 = 1500\n",
    "text_size_2 = 50\n",
    "\n",
    "x_transcript  = get_features(transc_joined, text_size_1)\n",
    "x_description = get_features(descr_joined, text_size_2)\n",
    "x_title = get_features(title_joined, text_size_2)\n",
    "\n",
    "y = np.array(df_topics_bin.iloc[:-rows_for_test])\n",
    "y = y.astype('int32')\n",
    "\n",
    "# Test\n",
    "\n",
    "\n",
    "x_transcript_test  = get_features(transc_joined_test, text_size_1)\n",
    "x_description_test = get_features(descr_joined_test, text_size_2)\n",
    "x_title_test = get_features(title_joined_test, text_size_2)\n",
    "x_test = [x_transcript_test,x_description_test] # , x_title_test\n",
    "\n",
    "y_test = np.array(df_topics_bin.iloc[-rows_for_test:])\n",
    "y_test = y_test.astype('int32')\n",
    "\n",
    "x_description.shape, x_transcript.shape,  y.shape, y_test.shape # x_title.shape,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "11/11 [==============================] - 7s 455ms/step - loss: 0.5152 - categorical_accuracy: 0.1341 - val_loss: 0.7678 - val_categorical_accuracy: 0.0722\n",
      "Epoch 2/30\n",
      "11/11 [==============================] - 4s 389ms/step - loss: 0.4587 - categorical_accuracy: 0.2211 - val_loss: 0.8241 - val_categorical_accuracy: 0.3952\n",
      "Epoch 3/30\n",
      "11/11 [==============================] - 4s 387ms/step - loss: 0.4430 - categorical_accuracy: 0.2280 - val_loss: 0.7803 - val_categorical_accuracy: 0.0722\n",
      "Epoch 4/30\n",
      "11/11 [==============================] - 4s 387ms/step - loss: 0.4382 - categorical_accuracy: 0.2257 - val_loss: 0.7074 - val_categorical_accuracy: 0.3746\n",
      "Epoch 5/30\n",
      "11/11 [==============================] - 4s 389ms/step - loss: 0.4350 - categorical_accuracy: 0.2188 - val_loss: 0.6506 - val_categorical_accuracy: 0.0722\n",
      "Epoch 6/30\n",
      "11/11 [==============================] - 4s 390ms/step - loss: 0.4328 - categorical_accuracy: 0.2073 - val_loss: 0.6181 - val_categorical_accuracy: 0.0722\n",
      "Epoch 7/30\n",
      "11/11 [==============================] - 4s 388ms/step - loss: 0.4311 - categorical_accuracy: 0.2073 - val_loss: 0.6383 - val_categorical_accuracy: 0.0722\n",
      "Epoch 8/30\n",
      "11/11 [==============================] - 4s 390ms/step - loss: 0.4267 - categorical_accuracy: 0.2199 - val_loss: 0.6397 - val_categorical_accuracy: 0.0722\n",
      "Epoch 9/30\n",
      "11/11 [==============================] - 4s 390ms/step - loss: 0.4177 - categorical_accuracy: 0.2559 - val_loss: 0.6213 - val_categorical_accuracy: 0.1924\n",
      "Epoch 10/30\n",
      "11/11 [==============================] - 4s 389ms/step - loss: 0.4034 - categorical_accuracy: 0.3115 - val_loss: 0.6046 - val_categorical_accuracy: 0.1959\n",
      "Epoch 11/30\n",
      "11/11 [==============================] - 4s 389ms/step - loss: 0.3861 - categorical_accuracy: 0.3820 - val_loss: 0.5937 - val_categorical_accuracy: 0.2680\n",
      "Epoch 12/30\n",
      "11/11 [==============================] - 4s 389ms/step - loss: 0.3728 - categorical_accuracy: 0.4448 - val_loss: 0.5891 - val_categorical_accuracy: 0.3540\n",
      "Epoch 13/30\n",
      "11/11 [==============================] - 4s 389ms/step - loss: 0.3582 - categorical_accuracy: 0.4471 - val_loss: 0.5709 - val_categorical_accuracy: 0.3677\n",
      "Epoch 14/30\n",
      "11/11 [==============================] - 4s 388ms/step - loss: 0.3463 - categorical_accuracy: 0.4655 - val_loss: 0.5651 - val_categorical_accuracy: 0.3780\n",
      "Epoch 15/30\n",
      "11/11 [==============================] - 4s 391ms/step - loss: 0.3353 - categorical_accuracy: 0.4962 - val_loss: 0.5498 - val_categorical_accuracy: 0.3608\n",
      "Epoch 16/30\n",
      "11/11 [==============================] - 4s 390ms/step - loss: 0.3242 - categorical_accuracy: 0.4950 - val_loss: 0.5510 - val_categorical_accuracy: 0.3574\n",
      "Epoch 17/30\n",
      "11/11 [==============================] - 4s 392ms/step - loss: 0.3153 - categorical_accuracy: 0.4981 - val_loss: 0.5381 - val_categorical_accuracy: 0.3677\n",
      "Epoch 18/30\n",
      "11/11 [==============================] - 4s 389ms/step - loss: 0.3054 - categorical_accuracy: 0.5142 - val_loss: 0.5000 - val_categorical_accuracy: 0.3952\n",
      "Epoch 19/30\n",
      "11/11 [==============================] - 4s 392ms/step - loss: 0.2966 - categorical_accuracy: 0.5092 - val_loss: 0.5034 - val_categorical_accuracy: 0.4089\n",
      "Epoch 20/30\n",
      "11/11 [==============================] - 4s 391ms/step - loss: 0.2875 - categorical_accuracy: 0.5352 - val_loss: 0.4885 - val_categorical_accuracy: 0.3918\n",
      "Epoch 21/30\n",
      "11/11 [==============================] - 4s 393ms/step - loss: 0.2785 - categorical_accuracy: 0.5395 - val_loss: 0.5012 - val_categorical_accuracy: 0.4055\n",
      "Epoch 22/30\n",
      "11/11 [==============================] - 4s 395ms/step - loss: 0.2725 - categorical_accuracy: 0.5498 - val_loss: 0.4823 - val_categorical_accuracy: 0.4124\n",
      "Epoch 23/30\n",
      "11/11 [==============================] - 4s 396ms/step - loss: 0.2638 - categorical_accuracy: 0.5510 - val_loss: 0.4719 - val_categorical_accuracy: 0.4330\n",
      "Epoch 24/30\n",
      "11/11 [==============================] - 4s 398ms/step - loss: 0.2545 - categorical_accuracy: 0.5705 - val_loss: 0.4575 - val_categorical_accuracy: 0.4124\n",
      "Epoch 25/30\n",
      "11/11 [==============================] - 4s 402ms/step - loss: 0.2473 - categorical_accuracy: 0.5828 - val_loss: 0.4495 - val_categorical_accuracy: 0.3814\n",
      "Epoch 26/30\n",
      "11/11 [==============================] - 4s 403ms/step - loss: 0.2378 - categorical_accuracy: 0.5770 - val_loss: 0.4442 - val_categorical_accuracy: 0.4021\n",
      "Epoch 27/30\n",
      "11/11 [==============================] - 4s 398ms/step - loss: 0.2304 - categorical_accuracy: 0.5835 - val_loss: 0.4485 - val_categorical_accuracy: 0.3780\n",
      "Epoch 28/30\n",
      "11/11 [==============================] - 4s 406ms/step - loss: 0.2203 - categorical_accuracy: 0.6065 - val_loss: 0.4425 - val_categorical_accuracy: 0.4124\n",
      "Epoch 29/30\n",
      "11/11 [==============================] - 5s 406ms/step - loss: 0.2111 - categorical_accuracy: 0.6126 - val_loss: 0.4409 - val_categorical_accuracy: 0.3814\n",
      "Epoch 30/30\n",
      "11/11 [==============================] - 4s 398ms/step - loss: 0.2019 - categorical_accuracy: 0.6268 - val_loss: 0.4520 - val_categorical_accuracy: 0.3746\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1b1fa5ec730>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input, Embedding\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Embedding, Flatten, GlobalMaxPool1D, Dropout, Conv1D, LSTM, Input, SpatialDropout1D\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.losses import binary_crossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "transcript_input = Input(shape = (None,), dtype = 'int64', name = 'transcription')\n",
    "description_input = Input(shape = (None,), dtype = 'int64', name = 'description')\n",
    "title_input = Input(shape = (None,), dtype = 'int64', name = 'title')\n",
    "\n",
    "embedded_transcript  = Embedding(input_dim = text_size_1, output_dim = 256, input_length=maxlen)(transcript_input)\n",
    "embedded_description = Embedding(input_dim = 50, output_dim = 32, input_length=50)(description_input)\n",
    "embedded_title = Embedding(input_dim = 50, output_dim = 32, input_length=50)(title_input)\n",
    "\n",
    "encoded_transcript  = Conv1D(filters = 256, kernel_size = 12, padding='valid', activation='sigmoid', strides=1)(embedded_transcript)\n",
    "encoded_description = Conv1D(filters = 32, kernel_size = 6, padding='valid', activation='sigmoid', strides=1)(embedded_description)\n",
    "encoded_title = Conv1D(filters = 32, kernel_size = 6, padding='valid', activation='sigmoid', strides=1)(embedded_title)\n",
    "\n",
    "pooled_transcript = GlobalMaxPool1D()(encoded_transcript)\n",
    "pooled_description = GlobalMaxPool1D()(encoded_description)\n",
    "pooled_title = GlobalMaxPool1D()(encoded_title)\n",
    "\n",
    "concat = layers.concatenate([pooled_transcript, pooled_description, pooled_title], axis = -1)\n",
    "# concat = layers.concatenate([pooled_transcript, pooled_description], axis = -1)\n",
    "\n",
    "\n",
    "x = Dense(500, activation = \"sigmoid\")(concat)\n",
    "\n",
    "output = Dense(num_classes, activation = \"sigmoid\")(x)\n",
    "\n",
    "model = Model([transcript_input, description_input, title_input], output)\n",
    "\n",
    "# model = Model([transcript_input, description_input], output)\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['categorical_accuracy'])\n",
    "\n",
    "\n",
    "callbacks = [\n",
    "    ReduceLROnPlateau(), \n",
    "    EarlyStopping(patience=4), \n",
    "    ModelCheckpoint(filepath='model-conv2d.h5', save_best_only = True)\n",
    "]\n",
    "\n",
    "model.fit([x_transcript, x_description, x_title], y, epochs = 30, batch_size=256, validation_split=0.1) #, callbacks = callbacks)\n",
    "# model.fit([x_transcript, x_description], y, epochs = 15, batch_size=256, validation_split=0.1) #, callbacks = callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "      science       0.57      0.60      0.59       151\n",
      "   technology       0.51      0.43      0.47        95\n",
      "      culture       0.29      0.52      0.38        29\n",
      "global issues       0.58      0.28      0.38        88\n",
      "      society       0.69      0.29      0.40        70\n",
      "       design       0.48      0.33      0.39        36\n",
      "social change       0.58      0.48      0.53        31\n",
      "    animation       0.54      0.33      0.41        95\n",
      "     business       0.55      0.39      0.46       111\n",
      "       health       1.00      0.00      0.01       202\n",
      "      history       0.59      0.37      0.45       104\n",
      "\n",
      "    micro avg       0.54      0.33      0.41      1012\n",
      "    macro avg       0.58      0.37      0.41      1012\n",
      " weighted avg       0.65      0.33      0.37      1012\n",
      "  samples avg       0.43      0.33      0.35      1012\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jakub\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# cnn_model = keras.models.load_model('model-conv2d.h5')\n",
    "\n",
    "predictions = model.predict([x_transcript_test, x_description_test, x_title_test])\n",
    "classes_predictions = prob_to_class(predictions)\n",
    "print(classification_report(y_test, classes_predictions, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jakub\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "keras_three = classification_report(y_test, classes_predictions, target_names=target_names, output_dict=True)\n",
    "f1_keras_three = np.array([keras_three[cat][\"f1-score\"] for cat in categories_names]).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "scores_df = pd.DataFrame([categories_names,pd.Series(f1_keras_one), pd.Series(f1_keras_one_w), pd.Series(f1_power),\\\n",
    "                          pd.Series(f1_chain),pd.Series(f1_keras_three), pd.Series(f1_brelevance)]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_df.columns = [\"Classes\",\"Keras one-input\", \"Keras one-input-weight\",\"S-k Powerset\",\"S-k Chain\", \"Keras three-input\",\"S-k B-relevance\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Classes</th>\n",
       "      <th>Keras one-input</th>\n",
       "      <th>Keras one-input-weight</th>\n",
       "      <th>S-k Powerset</th>\n",
       "      <th>S-k Chain</th>\n",
       "      <th>Keras three-input</th>\n",
       "      <th>S-k B-relevance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>science</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>technology</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>culture</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>global issues</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>society</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>design</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>social change</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>animation</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>business</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>health</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>history</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Classes Keras one-input Keras one-input-weight S-k Powerset  \\\n",
       "0         science             0.7                   0.65         0.75   \n",
       "1      technology            0.63                   0.62         0.63   \n",
       "2         culture             0.4                   0.44         0.52   \n",
       "3   global issues            0.23                   0.27         0.38   \n",
       "4         society            0.52                   0.63         0.25   \n",
       "5          design            0.52                   0.52         0.53   \n",
       "6   social change            0.52                   0.55         0.51   \n",
       "7       animation            0.37                   0.33         0.57   \n",
       "8        business            0.47                   0.47         0.23   \n",
       "9          health            0.95                   0.94         0.62   \n",
       "10        history            0.27                   0.25         0.42   \n",
       "\n",
       "   S-k Chain Keras three-input S-k B-relevance  \n",
       "0       0.68              0.59            0.54  \n",
       "1       0.59              0.47            0.44  \n",
       "2       0.41              0.38             0.2  \n",
       "3        0.2              0.38             0.3  \n",
       "4       0.34               0.4            0.12  \n",
       "5        0.5              0.39            0.13  \n",
       "6       0.47              0.53            0.24  \n",
       "7       0.48              0.41            0.15  \n",
       "8       0.25              0.46             0.1  \n",
       "9       0.78              0.01            0.12  \n",
       "10      0.34              0.45            0.23  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
